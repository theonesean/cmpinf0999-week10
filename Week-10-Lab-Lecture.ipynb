{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 Lab Lecture\n",
    "\n",
    "## Processing Data with Python, Part 2\n",
    "\n",
    "### Topics\n",
    "\n",
    "* exploring and summarizing data\n",
    "* data cleaning and manipulation\n",
    "* merging data\n",
    "* using time in pandas\n",
    "* working with WPRDC data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some more data manipulation\n",
    "\n",
    "There are a bunch of standard functions provided by pandas for manipulating data, and now that you've had a chance to get your feet wet, we're going to run through a bunch of things that you probably should know when doing data manipulation with pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More with exploring and summarizing data\n",
    "\n",
    "Once your data has been loaded as a Dataframe, you can start using Pandas various functions to quickly explore your data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load data\n",
    "center_attendance_pandas = pd.read_csv(\"community-center-attendance.csv\", \n",
    "                                       index_col=\"_id\") # use the column named _id as the row index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful functions for exploring DataFrames and Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for looking at parts of the DataFrame include\n",
    "* `<dataframe>.head(n)` - look at the first n rows of the dataframe\n",
    "* `<dataframe>.tail(n)` - look at the last n rows of the dataframe\n",
    "* `<dataframe>.sample(n)` - randomly select n rows from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first 10 rows\n",
    "center_attendance_pandas.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the last 5 rows\n",
    "center_attendance_pandas.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab 5 random rows\n",
    "center_attendance_pandas.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions that let us count rows and columns include: \n",
    "* `<dataframe>.shape` - return the rows and columns as a python data structure (not a function!)\n",
    "* `<dataframe>.info()` - Display the datatypes of the index and columns as well as memory usage\n",
    "* `<dataframe>.describe()` - Compute summary statistics for numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many rows and columns\n",
    "center_attendance_pandas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the datatypes\n",
    "center_attendance_pandas.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above shows us a lot of implementation details about our DataFrame. Data types, number of rows and columns, and the datatype of the column. It also shows us **memory usage**, which is useful because memory is a limited resource.\n",
    "\n",
    "From there, we can also start doing some computations on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary statistics on the numerical columns\n",
    "center_attendance_pandas.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `describe()` function will automatically compute summary statistics for numerical columns and ignore categorical columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Numerical Data\n",
    "\n",
    "One of the things I mentioned last week is that pandas doesn't do anything that plain old Python can't. We can use traditional Python functions to get information about our Dataframe.\n",
    "\n",
    "The `len()` function tells us the length of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a standard python function to get the length of the sequence\n",
    "len(center_attendance_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this tells us our dataset has 18,367 rows. But this is just information about the dataset itself, it doesn't tell us how many people visited community centers.\n",
    "\n",
    "What if we wanted to know the total attendance: how many people visited all the community centers for all time (in the dataset)?\n",
    "\n",
    "First, let's answer this using pure Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up the CSV module and center attendance in python data structures\n",
    "import csv\n",
    "\n",
    "with open('community-center-attendance.csv') as f:\n",
    "    center_attendance_python = [row for row in csv.reader(f)]\n",
    "\n",
    "# look at the first ten rows of the data loaded in python\n",
    "center_attendance_python[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable to hold the total attendance\n",
    "total_attendance = 0\n",
    "\n",
    "# loop over the data that was loaded using pure python\n",
    "for row in center_attendance_python[1:]: # skip the header row using a list slice\n",
    "    # add the row count to the total, convert string to int\n",
    "    row_attendance = int(row[3])\n",
    "    total_attendance = total_attendance + row_attendance\n",
    "\n",
    "print(total_attendance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, here is how we do the exact same thing with Pandas.\n",
    "\n",
    "This code selects the `attendance_count` column and then computes the sum of all the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the total attendance with the pandas sum function\n",
    "center_attendance_pandas['attendance_count'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the summary statistics individually, column-by-column.\n",
    "* `<dataframe>[<column name>].mean()` - calculate the mean value for the column values\n",
    "* `<dataframe>[<column name>].std()` - calculate the standard deviation for the column values\n",
    "* `<dataframe>[<column name>].var()` - calculate the variance value for the column values\n",
    "* `<dataframe>[<column name>].median()` - calculate the median value for the column values\n",
    "* `<dataframe>[<column name>].min()` - calculate the minimum value for the column values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean attendance per day at all community centers\n",
    "center_attendance_pandas['attendance_count'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation\n",
    "center_attendance_pandas['attendance_count'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance\n",
    "center_attendance_pandas['attendance_count'].var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median attendance per day at all community centers\n",
    "center_attendance_pandas['attendance_count'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum attendance at community centers\n",
    "center_attendance_pandas['attendance_count'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: missing values are automatically skipped unless the entire column is NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[1.4, None], [7.1, -4.5], \n",
    "                  [None, None], [0.75, -1.3]],\n",
    "                 index=['a','b','c','d'],\n",
    "                 columns=['one','two'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean(axis=1, skipna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math Operations\n",
    "\n",
    "You can do mathematical operations that will get applied to every value in the row or column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a numerical dataframe\n",
    "df = pd.DataFrame([[1.4, 4.7], [7.1, -4.5], \n",
    "                  [3, 7], [0.75, -1.3]],\n",
    "                 index=['a','b','c','d'],\n",
    "                 columns=['one','two'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple two columns against eachother\n",
    "df['one'] * df['two']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide a column by a number\n",
    "df['one'] / 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are, as you know, two major types of data: numerical and categorical. Pandas is not only a tool for working with numerical data. It has lots of functionality for manipulating categorical data, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Categorical Data\n",
    "\n",
    "Just like before, we can start counting the distribution of values in the column. \n",
    "\n",
    "How many entries do we have per community center? (This isn't counting attendance, it's counting the number of *attendance counts* per center).\n",
    "\n",
    "First, let's do this in pure Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the counts\n",
    "center_counter = dict()\n",
    "\n",
    "# loop over the data\n",
    "for row in center_attendance_python[1:]:\n",
    "    center = row[2]\n",
    "    \n",
    "    # check to see if the gender is already in the diction\n",
    "    if center not in center_counter:\n",
    "        # create a new entry\n",
    "        center_counter[center] = 1\n",
    "    else:\n",
    "        # increment a new entry\n",
    "        #center_counter[center] += 1\n",
    "        center_counter[center] = center_counter[center] + 1\n",
    "\n",
    "# Display the dictionary \n",
    "center_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas way, as usualy, is a bit easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same thing with pandas\n",
    "center_attendance_pandas['center_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_attendance_pandas['center_name'].value_counts().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a bunch of other functions for working with categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_attendance_pandas['center_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(center_attendance_pandas['center_name'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling, databases, and subsetting\n",
    "\n",
    "It is sometimes helpful to think of a Pandas DataFrame as a little database. There is data and information stored in the Pandas DataFrame (or Series) and you want to *retrieve* it.\n",
    "\n",
    "Pandas has multiple mechanisms for getting specific bits of data and information from its data structures. \n",
    "\n",
    "## Masking: Filtering by Values\n",
    "\n",
    "The most common is to use *masking* to select just the rows you want. \n",
    "\n",
    "Masking is a two stage process, first you create a sequence of boolean values based upon a conditional expression—which you can think of as a \"query\"—and then you index your dataframe using that boolean sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the chipotle order data\n",
    "order_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at all the columns\n",
    "order_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we only wanted to look at a **specific order**? Let's try and isolate just the orders for chicken bowls.\n",
    "\n",
    "Firstly, create a *query mask*, a list of `True/False` values for rows that satisfy a particular condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a query mask for chicken bowls\n",
    "query_mask = order_data['item_name'] == \"Chicken Bowl\"\n",
    "\n",
    "#look at the first 20 items to see what matches\n",
    "query_mask.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us the *row id* and True or False if the item is a chicken bowl.\n",
    "\n",
    "Let's just test to see if this is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_data.iloc[19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup! So now that we know the mask works, we can create a *subset* of our data containing chicken bowls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicken_bowls = order_data[query_mask]\n",
    "chicken_bowls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you've got just the orders for chicken bowls, so you can do data operations on *just* those orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean price for chicken bowls\n",
    "chicken_bowls['item_price'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how many chicken bowls people order\n",
    "chicken_bowls['quantity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine query masks using **boolean logic**. Can we look at just the chicken bowl orders that were less than $10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a query mask for chicken bowls\n",
    "item_query_mask = order_data['item_name'] == \"Chicken Bowl\"\n",
    "# create a query mask for cheap orders\n",
    "price_query_mask = order_data['item_price'] < 10\n",
    "\n",
    "# apply both query masks using boolean AND\n",
    "cheap_chicken_bowls = order_data[item_query_mask & price_query_mask]\n",
    "cheap_chicken_bowls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median price for cheap chicken bowls\n",
    "cheap_chicken_bowls['item_price'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query masks can be used to filter and create subsets of data.\n",
    "\n",
    "**NOTE**: this method of subsetting data creates what is called a \"view\" of the data. You are basically working with a big slice of the original dataframe, *not* a separate copy of the data.\n",
    "\n",
    "This means if you try an do transformations on that view, you will get an error. For more information, [see the pandas documentation](http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy).\n",
    "\n",
    "If you do want to do transformations on this data, it's trivial to make a copy, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheap_chicken_bowls['half_price'] = cheap_chicken_bowls['item_price'] / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_of_cheap_chicken_bowls = cheap_chicken_bowls.copy()\n",
    "copy_of_cheap_chicken_bowls['half_price'] = copy_of_cheap_chicken_bowls['item_price'] / 2\n",
    "copy_of_cheap_chicken_bowls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Data\n",
    "\n",
    "Bringing disparate datasets together is one of the more powerful features of pandas.\n",
    "\n",
    "Like with Python lists, you can `append()` and `concat()` pandas Series and Dataframes. The `concat` is a module function, you call it directly from the pandas module (usually called `pd`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatinate two series together\n",
    "ser1 = pd.Series(['A', 'B', 'C'], index=[1, 2, 3])\n",
    "ser2 = pd.Series(['D', 'E', 'F'], index=[4, 5, 6])\n",
    "pd.concat([ser1, ser2]) \n",
    "# note the Series are passed as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order matters\n",
    "pd.concat([ser2, ser1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatinate two dataframes\n",
    "df1 = pd.DataFrame({\"A\":[\"A1\", \"A2\"],\n",
    "                    \"B\":[\"B1\",\"B2\"]},index=[1,2])\n",
    "df2 = pd.DataFrame({\"A\":[\"A3\", \"A4\"],\n",
    "                    \"B\":[\"B3\",\"B4\"]},index=[3,4])\n",
    "pd.concat([df1,df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas will automatically handle lining up matching indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatinate dataframes horizontally\n",
    "df1 = pd.DataFrame({\"A\":[\"A1\", \"A2\"],\n",
    "                    \"B\":[\"B1\",\"B2\"]},index=[1,2])\n",
    "df2 = pd.DataFrame({\"C\":[\"C1\", \"C2\"],\n",
    "                    \"D\":[\"D1\",\"D2\"]},index=[1,2])\n",
    "pd.concat([df1,df2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And pandas will gracefully handle misalignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens when indexes don't line up\n",
    "df1 = pd.DataFrame({\"A\":[\"A1\", \"A2\"],\n",
    "                    \"B\":[\"B1\",\"B2\"]},index=[1,2])\n",
    "df2 = pd.DataFrame({\"A\":[\"A3\", \"A4\"],\n",
    "                    \"B\":[\"B3\",\"B4\"]},index=[3,4])\n",
    "pd.concat([df1,df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a hierarchical index\n",
    "df1 = pd.DataFrame({\"A\":[\"A1\", \"A2\"],\n",
    "                    \"B\":[\"B1\",\"B2\"]},index=[1,2])\n",
    "df2 = pd.DataFrame({\"A\":[\"A3\", \"A4\"],\n",
    "                    \"B\":[\"B3\",\"B4\"]},index=[3,4])\n",
    "pd.concat([df1,df2], keys=[\"df1\", 'df2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `append` function is a method of a Series/Dataframe and returns a new object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# append df2 to df1\n",
    "df1.append(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins: more powerful concatenation\n",
    "\n",
    "While `concat()` is useful, it lacks the power to do complex data merging.\n",
    "\n",
    "For example, let's say I have two tables of different data but one overlapping column.\n",
    "\n",
    "This is where the `merge()` function becomes useful because it lets you *join* datasets\n",
    "\n",
    "The concept of \"join\" has lots of theory and is a richly developed method for *joining* data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-to-one joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create two dataframes with one shared column\n",
    "df1 = pd.DataFrame({'employee': ['Bob', 'Jake', 'Lisa', 'Sue', \"Nancy\"],\n",
    "                    'group': ['Accounting', 'Engineering', 'Engineering', 'HR', \"Librarian\"]})\n",
    "df2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],\n",
    "                    'hire_date': [2004, 2008, 2012, 2014]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display df1\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display df2\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge df1 and df2 into a new dataframe df3\n",
    "df3 = pd.merge(df1, df2)\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new dataframe `df3` now has all of the data from df1 and df2.\n",
    "\n",
    "The `merge` function automatically connected the two tables on the \"employee\" column.\n",
    "\n",
    "But what happens when your data don't line up?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many-to-one joins\n",
    "\n",
    "Sometimes, there isn't a one to one relationshp between rows in the two datasets.\n",
    "\n",
    "A *many-to-one* join lets you combine these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make another dataframe about the supervisor for each group\n",
    "df4 = pd.DataFrame({'group': ['Accounting', 'Engineering', 'HR'],\n",
    "                    'supervisor': ['Carly', 'Guido', 'Steve']})\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df3 from above with the supervisor info in df4\n",
    "pd.merge(df3,df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the information about Guido, the manager for Engineering, is repeated.\n",
    "\n",
    "Pandas automatically fills in these values to maintain the tabular, 2 dimensional structure of the data.\n",
    "\n",
    "While this might seem like duplicated data, it makes it easier to quickly look up Jake and Lisa's supervisor without consulting multiple tables. It's sometimes better to duplicate data to have it arranged in a way that you want.\n",
    "\n",
    "Now, let's make it even more complicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many-to-many joins\n",
    "\n",
    "Let's combine the employee information with skills information.\n",
    "\n",
    "Notice there isn't a one-to-one or even a one-to-many relationship between these tables.\n",
    "\n",
    "Each group can have multiple skills, so **what do you think will happen?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the employee table specified above\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe with skills information\n",
    "df5 = pd.DataFrame({'group': ['Accounting', 'Accounting',\n",
    "                              'Engineering', 'Engineering', 'HR', 'HR', 'Librarian'],\n",
    "                    'skills': ['math', 'spreadsheets', 'coding', 'linux',\n",
    "                               'spreadsheets', 'organization', 'nunchucks']})\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's going to happen?\n",
    "pd.merge(df1, df5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing!\n",
    "\n",
    "![whoa dude](https://media.giphy.com/media/Lcn0yF1RcLANG/giphy.gif)\n",
    "\n",
    "Pandas merge capabilities are very useful.\n",
    "\n",
    "But what do you do if the names of your columns don't match? You could change column names.\n",
    "\n",
    "But that's crazy! Just use the `left_on` and `right_on` parameters to the `merge()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the employee table specified above\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new salary table, but use \"name\" instead of \"employee\" for the column index\n",
    "df3 = df3 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue','Nancy'],\n",
    "                    'salary': [70000, 80000, 120000, 90000,1000000]})\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets try and merge them without specifying what to merge on\n",
    "pd.merge(df1, df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the column names I should specify here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets specify the column name \n",
    "pd.merge(df1, df3, left_on=\"employee\", right_on=\"name\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we now have a redundant employee/name column; this is a by-product of merging different columns.\n",
    "\n",
    "If you want to get rid of it, that's trivial: you can use the `drop` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the name column, axis=1 means axis='col', which is confusing\n",
    "pd.merge(df1, df3, left_on=\"employee\", right_on=\"name\" ).drop(\"name\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like I said, there is a ton of theory around merging and joining data, so this is just us dipping our toes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `groupby()` and aggregation\n",
    "\n",
    "We looked at the `groupby()` function last week, and I don't want to waste time on it today. Essentially, `groupby()` lets you group all of the items in a DataFrame by a single column.\n",
    "\n",
    "There's a bunch of other ways to sort data in DataFrames with pandas. The following table summarizes some other built-in Pandas aggregations:\n",
    "\n",
    "| Aggregation              | Description                     |\n",
    "|--------------------------|---------------------------------|\n",
    "| ``count()``              | Total number of items           |\n",
    "| ``size()``               | Total number of items w/ NaNs   |\n",
    "| ``first()``, ``last()``  | First and last item             |\n",
    "| ``mean()``, ``median()`` | Mean and median                 |\n",
    "| ``min()``, ``max()``     | Minimum and maximum             |\n",
    "| ``std()``, ``var()``     | Standard deviation and variance |\n",
    "| ``mad()``                | Mean absolute deviation         |\n",
    "| ``prod()``               | Product of all items            |\n",
    "| ``sum()``                | Sum of all items                |\n",
    "\n",
    "These are all functions of ``DataFrame`` and ``Series`` objects.\n",
    "\n",
    "You can also do multiple levels of grouping with something called [Multilevel Indexing](https://pandas.pydata.org/pandas-docs/stable/advanced.html). Unfortunately, we don't have time to go in depth into this, but the Python Data Science Handbook (one of your textbooks for the course) has a great intro to the topic in the chapter [Hierarchical Indexing](https://jakevdp.github.io/PythonDataScienceHandbook/03.05-hierarchical-indexing.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, let's work with some real data\n",
    "\n",
    "The WPRDC, which stands for Western Pennsylvania Regional Data Center, is *the* place to go for data around Pittsburgh. You'll be working with WPRDC data for your final project (*wink, wink*).\n",
    "\n",
    "So, let's grab the [Allegheny County Jail's daily census](https://data.wprdc.org/dataset/allegheny-county-jail-daily-census) from the WPRDC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grab three months of data\n",
    "january17_jail_census = pd.read_csv(\"https://data.wprdc.org/datastore/dump/3b5d9c45-b5f4-4e05-9cf1-127642ad1d17\")\n",
    "feburary17_jail_census = pd.read_csv(\"https://data.wprdc.org/datastore/dump/cb8dc876-6285-43a8-9db3-90b84eedb46f\")\n",
    "march17_jail_census = pd.read_csv(\"https://data.wprdc.org/datastore/dump/68645668-3f89-4831-b1de-de1e77e52dd3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "january17_jail_census.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the concat function to combine all three into one dataframe\n",
    "# Remember I need to make a list of the all the dataframes for\n",
    "# the concat fuction\n",
    "jail_census = pd.concat([january17_jail_census, \n",
    "                         feburary17_jail_census, \n",
    "                         march17_jail_census])\n",
    "jail_census"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though these aren't *really* valid computations because we are looking at a daily census, we can still use these data for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the average age at booking by gender\n",
    "jail_census.groupby('Gender')['Age at Booking'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute the average age at booking by race then gender \n",
    "jail_census.groupby(['Race', 'Gender'])['Age at Booking'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the [data dictionary](https://data.wprdc.org/dataset/allegheny-county-jail-daily-census/resource/f0550174-16b0-4f6e-88dc-fa917e74b56c), we can see the following mapping for race categories:\n",
    "```\n",
    "Race of Inmate\n",
    "A-ASIAN OR PACIFIC ISLANDER\n",
    "B-BLACK OR AFRICAN AMERICAN\n",
    "H-HISPANIC \n",
    "I-AMERICAN INDIAN OR ALASKAN NATIVE\n",
    "U-UNKNOWN\n",
    "W-WHITE\n",
    "```\n",
    "The `x` category hasn't been described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how many total rows in the dataset have \"x\" for race\n",
    "jail_census['Race'].value_counts()['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the statistical summary of age at booking by gender\n",
    "jail_census.groupby('Gender')['Age at Booking'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the difference between Age at Booking and current age\n",
    "age_difference = jail_census['Current Age'] - jail_census['Age at Booking']\n",
    "age_difference.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sort them by date, so we can see who was there on any given day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jail_census.groupby('Date').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jail_census['year'] = jail_census['Date'].str.split(\"-\").str[0]\n",
    "jail_census['month'] = jail_census['Date'].str.split(\"-\").str[1]\n",
    "jail_census['day'] = jail_census['Date'].str.split(\"-\").str[2]\n",
    "\n",
    "jail_census.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jail_census.groupby('month').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jail_census.groupby('day').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a really awkward way of dealing with time. We shouldn't have to make a separate column for year, month, day.\n",
    "\n",
    "There must be a better way to do this time stuff... any ideas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One more thing\n",
    "\n",
    "Before we get into doing some real-world data manipulation, I want to briefly dive into working with time in pandas. One of the most powerful features of pandas is its time series functionality.\n",
    "\n",
    "Dates and time are a Python and pandas data type (like integers and strings). By using the `datetime` data types you can do advanced, time-centric analysis.\n",
    "\n",
    "One thing to remember about computers is they are *very* specific. The following things are *all different* to the computer:\n",
    "\n",
    "* **time stamps** - a specific moment in time (July 4th, 2017 at 7:52am and 34 seconds)\n",
    "* **time intervals** - a length of time with start and end points (the year 2017)\n",
    "* **time duration** - a specific length of time (a year, a month, a day)\n",
    "   \n",
    "Pandas has its own data types for time (much like Series and DataFrame). If you have a lot of dates, it is often useful to use the Pandas functions over the native Python functions. Pandas is most powerful when you index by time using the `DatetimeIndex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Series with a DateTime index\n",
    "index = pd.DatetimeIndex(['2014-03-04', '2014-08-04',\n",
    "                          '2015-04-04', '2015-09-04',\n",
    "                          '2016-01-01', '2016-02-16'])\n",
    "data = pd.Series([0, 1, 2, 3, 4, 5], index=index)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the index is made of DateTimes, we can index using date strings.\n",
    "\n",
    "**NOTE**: this only works on strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the value for a specific day\n",
    "data[\"2015-04-04\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab a slice between two dates\n",
    "data['2014-08-01':'2016-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give me everything from 2015\n",
    "data['2015']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has some functions to make parsing dates easy, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the to_datetime function instead of the parser function\n",
    "date = pd.to_datetime(\"4th of July, 2017\")\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use string format codes to get the weekday\n",
    "date.strftime(\"%A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give me today's date\n",
    "today = pd.to_datetime(\"today\")\n",
    "today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is the day, but also the exact time. Remember? Computers are picky. Timestamps must always be a *specific moment*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Time on Real Data\n",
    "\n",
    "Let's look at the [311 data for the city of Pittsburgh](https://data.wprdc.org/dataset/311-data) from the WPRDC. Did you know, you can give the URL directly to Pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the 311 data directly from the WPRDC\n",
    "pgh_311_data = pd.read_csv(\"https://data.wprdc.org/datastore/dump/76fda9d0-69be-4dd5-8108-0de7907fc5a4\")\n",
    "pgh_311_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the dataframe and Pandas automatic data type detection\n",
    "pgh_311_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we have the data, but we need it to be indexed by date.\n",
    "\n",
    "* **What column has the date information?**\n",
    "* **What format do you think that column is currently in?**\n",
    "* **What function might we use to convert that column into dates?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgh_311_data['CREATED_ON'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the \"CREATED_ON\" column to dates\n",
    "pd.to_datetime(pgh_311_data['CREATED_ON']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert the \"CREATED_ON\" column to Pandas `datetime` objects, and now we have to set that to the dataframe's index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the index of pgh_311_data to be the parsed dates in the \"CREATED_ON\" column\n",
    "pgh_311_data.index = pd.to_datetime(pgh_311_data['CREATED_ON'])\n",
    "pgh_311_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh, now we have CREATED_ON twice. That isn't very tidy. We can also skip this extra conversion step entirely by specifying the index column and date parsing in `read_csv()` function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the 311 data directly from the WPRDC and parse dates directly\n",
    "pgh_311_data = pd.read_csv(\"https://data.wprdc.org/datastore/dump/76fda9d0-69be-4dd5-8108-0de7907fc5a4\",\n",
    "                           index_col=\"CREATED_ON\", \n",
    "                           parse_dates=True)\n",
    "pgh_311_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgh_311_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataframe has been indexed by time, we can select 311 complains by time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 311 complaints on January 1st, 2016\n",
    "pgh_311_data['2016-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the times just around the new years celebration\n",
    "pgh_311_data[\"2015-12-31 20:00:00\":\"2016-01-01 02:00:00\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Someone had a fun New Year's. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "\n",
    "Last week, we \"smoothed over\" the community center data by using the `resample()` method. Let's do that some more, shall we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of complaints per month\n",
    "pgh_311_data.resample(\"W\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean of complaints per quarter...note this doesn't make sense, but works anyway\n",
    "pgh_311_data.resample(\"Q\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tell matplotlib to render plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph of the monthly complaint counts\n",
    "pgh_311_data['REQUEST_ID'].resample(\"M\").count().plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the code above, but re-sampling based upon different date periods. The strings for specifying an offset are located [here](http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases) and below:\n",
    "\n",
    "|Alias|Description|\n",
    "|-----|-----------|\n",
    "|B|business day frequency|\n",
    "|C|custom business day frequency|\n",
    "|D|calendar day frequency|\n",
    "|W|weekly frequency|\n",
    "|M|month end frequency|\n",
    "|SM|semi-month end frequency (15th and end of month)|\n",
    "|BM|business month end frequency|\n",
    "|CBM|custom business month end frequency|\n",
    "|MS|month start frequency|\n",
    "|SMS|semi-month start frequency (1st and 15th)|\n",
    "|BMS|business month start frequency|\n",
    "|CBMS|custom business month start frequency|\n",
    "|Q|quarter end frequency|\n",
    "|BQ|business quarter end frequency|\n",
    "|QS|quarter start frequency|\n",
    "|BQS|business quarter start frequency|\n",
    "|A, Y|year end frequency|\n",
    "|BA, BY|business year end frequency|\n",
    "|AS, YS|year start frequency|\n",
    "|BAS, BYS|business year start frequency|\n",
    "|BH|business hour frequency|\n",
    "|H|hourly frequency|\n",
    "|T, min|minutely frequency|\n",
    "|S|secondly frequency|\n",
    "|L, ms|milliseconds|\n",
    "|U, us|microseconds|\n",
    "|N|nanoseconds|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try a different resampling here\n",
    "\n",
    "pgh_311_data['REQUEST_ID'].resample(\"SOMETHING DIFFERENT\").count().plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try yet another resampling here\n",
    "\n",
    "pgh_311_data['REQUEST_ID'].resample(\"EVEN MORE DIFFERENT\").count().plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping up\n",
    "\n",
    "Okay, so that's pandas. We'll get a little bit more into some specifics next week when we talk about data visualization. We'll be using pandas, geopandas, and matplotlib to answer some interesting questions with data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nteract": {
   "version": "0.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
